# DecIF: Improving Instruction-Following through Decomposition
This repository contains source code of our paper: **DecIF: Improving Instruction-Following through Decomposition**

## Training Data
We release our 10k and 30k instruction-following data generated by LLaMA-3.1-70B-Instruct. You can directly download the data and training your own models or mix with other domains data.

```
the link of our released data
```

If you want to construct the training data using your own supervised models, you can follow the following guide to run DecIF and get your own high-quality instruction-following data.

## Environment Preparation
Please follow the LLaMA-Factory repository to prepare the environment (Note that you should install vLLM for efficiently decoding).

## Workflow of DecIF
### 1. Generate Meta Domains
Excude the following python code to generate meta domains:

```
python 1_generate_domain.py \
    --model_name_or_path <model_name_or_path> \
    --output_file <txt file of meta domains>
```

### 2. Generate Meta Requests
Excude the following python code to generate meta requests:

```
python 2_generate_meta_request.py \
    --model_name_or_path <model_name_or_path> \
    --domain_file <txt file of meta domains> \
    --output_file <txt file of meta requests>
```

### 3. Generate Meta Scenarios
Excude the following python code to generate meta scenarios:

```
python 3_generate_scenario.py \
    --model_name_or_path <model_name_or_path> \
    --input_file <txt file of meta requests> \
    --output_file <json file of meta scenarios>
```

### 4. Generate Initial Instructions
Excude the following python code to generate initial instructions with constraints:

```
python 4_generate_instruction_with_constraint.py \
    --model_name_or_path <model_name_or_path> \
    --input_json <json file of meta scenarios> \
    --output_json <json file of initial instructions> \
    --total_num <the number of generated instructions>
```

Or you can also generate the general-purpose instruction data using the following command:

```
python 4_generate_wild_instruction.py \
    --model-path <model_name_or_path> \
    --input-json <json file of meta scenarios> \
    --output-json <json file of initial instructions> \
    --total_num <the number of generated instructions>
```

### 5. Judge Instruction Consistency
Excude the following python code to refine the instructions:

```
python 5_judge_query_consistency.py \
    --model_name_or_path <model_name_or_path> \
    --input_file <json file of initial instructions> \
    --output_file <json file of refined instructions>
```

### 6. Generate Responses
Excude the following python code to generate the responses:

```
python 5_generate_response.py \
    --model_name_or_path <model_name_or_path> \
    --input_json <json file of refined instructions> \
    --output_json <json file of the instruction-response pairs> 
```

### 7. Generate Criteria
Excude the following python code to generate the criteria for instructions:

```
python 7_generate_criteria.py \
    --model_path $model_name_or_path \
    --input_file <json file of the instruction-response pairs> \
    --output_file <json file of the criteria>
```

### 8. Judge Responses
Excude the following python code to judge the responses:

```
python 8_judge_response.py \
    --model_path <model_name_or_path> \
    --input_file <json file of the criteria> \
    --output_file <json file of the judgements> 
```

### 9. Filter Responses
Excude the following python code to filter the instruction data (you should manually modify the file_path to save your final instruction data):

```
python 9_filter_response.py
```

### 10. Get the LLaMA-Factory Training Data (Optional)
Excude the following python code to get the training data properly for LLaMA-Factory framework:

```
python 10_get_factory_data.py \
    --input <json file of the final data> \
    --output <jsonl file for training>
```

## Training Script in LLaMA-Factory
You can follow the guide of LLaMA-Factory framework to start training. We provide an example of the training scripts:

```
torchrun --nproc_per_node=8 src/train.py \
    --model_name_or_path <model_name_or_path> \
    --flash_attn fa2 \
    --template <proper chat template of the model> \
    --dataset <dataset name in dataset_info.json> \
    --cutoff_len 8192 \
    --do_train True \
    --stage sft \
    --finetuning_type full \
    --preprocessing_num_workers 64 \
    --overwrite_cache True \
    --output_dir <file path to save the models> \
    --logging_steps 1 \
    --save_strategy epoch \
    --save_total_limit 1 \
    --plot_loss True \
    --overwrite_output_dir True \
    --save_only_model True \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --warmup_ratio 0.03 \
    --learning_rate 1.0e-5 \
    --num_train_epochs 3.0 \
    --lr_scheduler_type cosine \
    --bf16 True \
    --ddp_timeout 180000000 \
    --gradient_checkpointing True \
    --deepspeed <config of deepspeed zero> \
    --report_to none 
```

## Citation
If this work is helpful, please kindly cite as:

```
the paper's citation bibtext
```